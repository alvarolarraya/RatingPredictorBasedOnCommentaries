{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37044201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta funcion obtengo los datos de una url que recibo por parametro.\n",
    "def getData(url):\n",
    "    headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "    r = req.get(url,headers = headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Con esta funcion obtengo el link de la sieguiente pagina de reseñas.\n",
    "def getNextPage(soup):\n",
    "    page = soup.find('ul',{'class': 'a-pagination'})\n",
    "    if not page.find('li', {'class':'a-disabled a-last'}):\n",
    "        url = 'https://www.amazon.es' + str(page.find('li',{'class':'a-last'}).find('a')['href'])\n",
    "        return url\n",
    "    else :\n",
    "        return\n",
    "\n",
    "# Con esta funcion leemos todos los comentarios de una pagina\n",
    "def getComentarios(soup):\n",
    "    comentarios = []\n",
    "    aux = soup.findAll('span',{'class': 'a-size-base review-text review-text-content'})\n",
    "    for i in aux:\n",
    "        comentarios.append(i.get_text(strip=True))\n",
    "    return comentarios\n",
    "\n",
    "def getEstrellas(soup):\n",
    "    estrellas = []\n",
    "    #de la pagina me quedo solo con la parte donde estan los comentarios (no nos interesa la media\n",
    "    # de estrellas por ejemplo)\n",
    "    aux = soup.findAll('div',{'class': 'a-section celwidget'})\n",
    "    for i in aux:\n",
    "        iconoEstrellas = i.find('span',{'class': 'a-icon-alt'})\n",
    "        estrellas.append(re.sub(',\\d de 5 estrellas','',iconoEstrellas.get_text(strip=True))) \n",
    "    return estrellas\n",
    "\n",
    "def tokenizar(comentario):\n",
    "    tokensComentario = word_tokenize(comentario)\n",
    "    return tokensComentario\n",
    "\n",
    "def crearCorpus(listadoResenias):\n",
    "    ps = PorterStemmer()\n",
    "    for i,resenia in enumerate(listadoResenias):\n",
    "        tokensResenia = tokenizar(resenia)\n",
    "        reseniaStemmizada = \"\"\n",
    "        for token in tokensResenia:\n",
    "            tokenStemmizado = ps.stem(token)\n",
    "            reseniaStemmizada += \" \"+tokenStemmizado\n",
    "        listadoResenias[i] = reseniaStemmizada\n",
    "    return listadoResenias\n",
    "\n",
    "# Con esta funcion leemos todas las reseñas del libro de la url.\n",
    "def leerLibro(url):\n",
    "    soup = getData(url)\n",
    "    todosLosComentarios = []\n",
    "    todasLasEstrellas = []\n",
    "    while (True):\n",
    "        soup = getData(url)\n",
    "        todosLosComentarios.append(getComentarios(soup))\n",
    "        todasLasEstrellas.append(getEstrellas(soup))\n",
    "        url = getNextPage(soup)\n",
    "        if not url:\n",
    "            break\n",
    "    return todosLosComentarios,todasLasEstrellas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
